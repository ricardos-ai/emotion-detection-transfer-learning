{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 05 - In-Class Competition -  Emotion Detection\n",
    "\n",
    "You've seen the power of transfer learning in building new models; let's try it again. You're going to build a model to predict someone's emotion based on a photo of their face.\n",
    "\n",
    "<img src=\"./images/emotions.png\" style=\"width:50%; margin:0 auto;\" />\n",
    "\n",
    "\n",
    "__Dataset summary:__\n",
    "\n",
    "The dataset we use contains about 12k labeled images. Each image represents only one facial emotion. The dataset contains seven different classes of emotions: namely: Surprise, Fear, Disgust, Happiness, Sadness, Anger, and Neutral.\n",
    "\n",
    "__Objective:__\n",
    "\n",
    "Your task is to build an image classification model using PyTorch's pre-trained models that classifies which class of emotion a human is showing.\n",
    "\n",
    "Here are the steps you need to follow:\n",
    "\n",
    "__Step 1: Preparing Data__\n",
    "\n",
    "Follow the steps below to create the PyTorch dataset.\n",
    "\n",
    "__Step 2: PyTorch DataLoaders__\n",
    "\n",
    "Using the created datasets, build training and validation DataLoaders.\n",
    "\n",
    "__Step 3: Building the Model__\n",
    "\n",
    "Create your model with the help of one of PyTorch's pretrained models. e.g. `resnet50`.\n",
    "\n",
    "__Step 4: Training and Evaluation__\n",
    "\n",
    "Train your model using the train dataset and validate it on the validation dataset.\n",
    "\n",
    "__Step 5: Predictions__\n",
    "\n",
    "Time to put your trained model to the test. This one is on me!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is D6D3-CFD0\n",
      "\n",
      " Directory of C:\\Users\\ricar\\Desktop\\compe\n",
      "\n",
      "07/03/2025  08:41    <DIR>          .\n",
      "07/03/2025  08:28    <DIR>          ..\n",
      "07/03/2025  08:41    <DIR>          .ipynb_checkpoints\n",
      "06/03/2025  21:16            17,913 Homework 05 -- Emotion Detection.ipynb\n",
      "               1 File(s)         17,913 bytes\n",
      "               3 Dir(s)  843,109,957,632 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:59:49.699793Z",
     "iopub.status.busy": "2025-03-07T07:59:49.699500Z",
     "iopub.status.idle": "2025-03-07T07:59:49.706696Z",
     "shell.execute_reply": "2025-03-07T07:59:49.705988Z",
     "shell.execute_reply.started": "2025-03-07T07:59:49.699772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7c177520b630>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "np.random.seed(72)\n",
    "torch.manual_seed(72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Once you know that you have prepared the data as I asked, and that you have a a working model and code, you can switch to Kaggle or Colab to train the model on GPUs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:52:51.189749Z",
     "iopub.status.busy": "2025-03-07T07:52:51.189534Z",
     "iopub.status.idle": "2025-03-07T07:52:51.270157Z",
     "shell.execute_reply": "2025-03-07T07:52:51.269015Z",
     "shell.execute_reply.started": "2025-03-07T07:52:51.189730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b7c857a8efef>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and Preprocessing\n",
    "\n",
    "Let's see what we have!\n",
    "\n",
    "`train_all` a directory contains 12,000 face images!\n",
    "\n",
    "`train_labels.csv` a csv files contains a list of image names and their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_517.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_11953.jpg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_8246.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_7711.jpg</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_9351.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img  label\n",
       "0    img_517.jpg      4\n",
       "1  img_11953.jpg      7\n",
       "2   img_8246.jpg      1\n",
       "3   img_7711.jpg      4\n",
       "4   img_9351.jpg      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_labels.csv')\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to be able to use TorchVision ImageFolder for datasets (which is the most convenient way to handle image datasets), we need to create a directory for each class of images and then move each image into its corresponding directory. \n",
    "\n",
    "Of course we do not want to do this manually!\n",
    "\n",
    "<div class=\"alert-info\">\n",
    "If you do not know how, ask ChatGPT! to write some code to do this for you. <br />\n",
    "Yes! this is the first time I am asking you to use it, and it's not like you haven't used it before! ;)\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "After this step, you should have a directory called __train__, where inside there are 7 directories named based on each class of activity, and the corresponding images are stored inside. Something like this:\n",
    "\n",
    "<pre>\n",
    "├── train\n",
    "  ├── Surprise\n",
    "  │   ├── img_5.jpg\n",
    "  │   ├── img_9.jpg\n",
    "  │   └── ...\n",
    "  ├── Fear\n",
    "  │   ├── img_26.jpg\n",
    "  │   ├── img_613.jpg\n",
    "  │   └── ...\n",
    "  └── ...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_class = { 1: \"Surprise\", 2: \"Fear\", 3: \"Disgust\", 4: \"Happiness\", \n",
    "                    5: \"Sadness\", 6: \"Anger\", 7: \"Neutral\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got it right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:53:19.396867Z",
     "iopub.status.busy": "2025-03-07T07:53:19.396542Z",
     "iopub.status.idle": "2025-03-07T07:53:19.400846Z",
     "shell.execute_reply": "2025-03-07T07:53:19.399791Z",
     "shell.execute_reply.started": "2025-03-07T07:53:19.396840Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:53:17.481042Z",
     "iopub.status.busy": "2025-03-07T07:53:17.480766Z",
     "iopub.status.idle": "2025-03-07T07:53:17.490159Z",
     "shell.execute_reply": "2025-03-07T07:53:17.489379Z",
     "shell.execute_reply.started": "2025-03-07T07:53:17.481020Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=100x100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = Image.open('/kaggle/input/faceemotions/FaceEmotions/train/Surprise/img_246.jpg')\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two TorchVision transforms, one for the train and one for the validation dataset.\n",
    "\n",
    "<span style=\"color:crimson\">Note:</span>\n",
    "\n",
    "    - The train transform should have at least these four operations.\n",
    "    - The validation transform should have only these four operations and nothing else.\n",
    "\n",
    "- Resize the image to 256. \n",
    "- CenterCrop image to 224. The size needed for __restnet50__ model.\n",
    "- Convert to tensors.\n",
    "- Normalize with: [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    \n",
    "name them __train_transform__ and __val_transform__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:53:46.931922Z",
     "iopub.status.busy": "2025-03-07T07:53:46.931583Z",
     "iopub.status.idle": "2025-03-07T07:53:46.935483Z",
     "shell.execute_reply": "2025-03-07T07:53:46.934587Z",
     "shell.execute_reply.started": "2025-03-07T07:53:46.931895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:54:34.958110Z",
     "iopub.status.busy": "2025-03-07T07:54:34.957803Z",
     "iopub.status.idle": "2025-03-07T07:54:35.098074Z",
     "shell.execute_reply": "2025-03-07T07:54:35.097389Z",
     "shell.execute_reply.started": "2025-03-07T07:54:34.958084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         ...,\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         ...,\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         ...,\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforms | Augmentation\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(256),\n",
    "        transforms.RandomRotation(degrees=45),\n",
    "        transforms.ColorJitter(hue=0.5),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( mean=[0.485, 0.456, 0.406] , std=[0.229, 0.224, 0.225] )\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize( mean=[0.485, 0.456, 0.406] , std=[0.229, 0.224, 0.225] )\n",
    "    ]\n",
    ")\n",
    "train_transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataset named __train_dataset__ using TorchVIsion ImageFolder for your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:55:54.034783Z",
     "iopub.status.busy": "2025-03-07T07:55:54.034462Z",
     "iopub.status.idle": "2025-03-07T07:56:06.731012Z",
     "shell.execute_reply": "2025-03-07T07:56:06.730358Z",
     "shell.execute_reply.started": "2025-03-07T07:55:54.034760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "train_dataset = datasets.ImageFolder(root=\"/kaggle/input/faceemotions/FaceEmotions/train/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:56:11.102971Z",
     "iopub.status.busy": "2025-03-07T07:56:11.102685Z",
     "iopub.status.idle": "2025-03-07T07:56:11.107762Z",
     "shell.execute_reply": "2025-03-07T07:56:11.106994Z",
     "shell.execute_reply.started": "2025-03-07T07:56:11.102949Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:56:13.823127Z",
     "iopub.status.busy": "2025-03-07T07:56:13.822844Z",
     "iopub.status.idle": "2025-03-07T07:56:13.828161Z",
     "shell.execute_reply": "2025-03-07T07:56:13.827259Z",
     "shell.execute_reply.started": "2025-03-07T07:56:13.823105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anger': 0,\n",
       " 'Disgust': 1,\n",
       " 'Fear': 2,\n",
       " 'Happiness': 3,\n",
       " 'Neutral': 4,\n",
       " 'Sadness': 5,\n",
       " 'Surprise': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = train_dataset.class_to_idx\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:crimson\"> Important! </span>\n",
    "After creating the dataset using ImageFolder, the labels are now:\n",
    "\n",
    "{'Anger': 0,\n",
    " 'Disgust': 1,\n",
    " 'Fear': 2,\n",
    " 'Happiness': 3,\n",
    " 'Neutral': 4,\n",
    " 'Sadness': 5,\n",
    " 'Surprise': 6}\n",
    " \n",
    "Make sure you have the same mapping between class names and class indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the __train_dataset__ to create train and validation datasets. Use __11000__ examples for the train and __1000__ examples for the validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:58:19.704006Z",
     "iopub.status.busy": "2025-03-07T07:58:19.703694Z",
     "iopub.status.idle": "2025-03-07T07:58:19.713094Z",
     "shell.execute_reply": "2025-03-07T07:58:19.712368Z",
     "shell.execute_reply.started": "2025-03-07T07:58:19.703985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=100x100>, 0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T07:59:57.705876Z",
     "iopub.status.busy": "2025-03-07T07:59:57.705582Z",
     "iopub.status.idle": "2025-03-07T07:59:57.719529Z",
     "shell.execute_reply": "2025-03-07T07:59:57.718550Z",
     "shell.execute_reply.started": "2025-03-07T07:59:57.705854Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sum of input lengths does not equal the length of the input dataset!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0f7bc435925e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m11000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths, generator)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0;31m# Cannot verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    481\u001b[0m             \u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Sum of input lengths does not equal the length of the input dataset!"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split(train_dataset, [11000, 1000])\n",
    "\n",
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:crimson\">Note:</span> If your train and validation transforms are different. you can assign the correct ones to your datasets here after the split. Assign __train_transform__ to __train_dataset__ and __val_transform__ to __val_dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T08:01:48.296247Z",
     "iopub.status.busy": "2025-03-07T08:01:48.295942Z",
     "iopub.status.idle": "2025-03-07T08:01:48.299783Z",
     "shell.execute_reply": "2025-03-07T08:01:48.298965Z",
     "shell.execute_reply.started": "2025-03-07T08:01:48.296226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the created datasets, build training and validation DataLoaders. Select an appropriate __batch_size__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:18:12.988302Z",
     "iopub.status.busy": "2025-03-07T09:18:12.987938Z",
     "iopub.status.idle": "2025-03-07T09:18:13.301331Z",
     "shell.execute_reply": "2025-03-07T09:18:13.299784Z",
     "shell.execute_reply.started": "2025-03-07T09:18:12.988272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-56-3a4d40804541>:1: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Enables mixed precision\n",
      "<ipython-input-56-3a4d40804541>:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Enables float16 precision\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 27.12 MiB is free. Process 2796 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 56.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-3a4d40804541>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Enables float16 precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-039be4409143>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Initialize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 27.12 MiB is free. Process 2796 has 15.86 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 56.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()  # Enables mixed precision\n",
    "\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    with torch.cuda.amp.autocast():  # Enables float16 precision\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:18:21.321143Z",
     "iopub.status.busy": "2025-03-07T09:18:21.320816Z",
     "iopub.status.idle": "2025-03-07T09:18:21.325591Z",
     "shell.execute_reply": "2025-03-07T09:18:21.324757Z",
     "shell.execute_reply.started": "2025-03-07T09:18:21.321118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Reduce batch size to avoid OOM errors\n",
    "batch_size = 16  # Try 16, if still OOM, use 8\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:17:48.406493Z",
     "iopub.status.busy": "2025-03-07T09:17:48.406277Z",
     "iopub.status.idle": "2025-03-07T09:17:48.416386Z",
     "shell.execute_reply": "2025-03-07T09:17:48.415726Z",
     "shell.execute_reply.started": "2025-03-07T09:17:48.406469Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 8)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the input, output dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:17:48.985857Z",
     "iopub.status.busy": "2025-03-07T09:17:48.985592Z",
     "iopub.status.idle": "2025-03-07T09:17:49.230489Z",
     "shell.execute_reply": "2025-03-07T09:17:49.229737Z",
     "shell.execute_reply.started": "2025-03-07T09:17:48.985827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = next(iter(val_loader))\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what I told you about how we build models in transfer learning (you can also check the corresponding code file), build a classification model using `resnet50` to classify our images.\n",
    "\n",
    "Here are the steps you can follow:\n",
    "\n",
    "- Bring `resnet50` from torchvision models.\n",
    "- Review the model layers and architecture. (It is a very large model, trained to classify 1000 image labels, so it can be used as a powerful feature extractor).\n",
    "- The only part of the model I want you to change is the __fc__ part at the end. Replace it with your own __fc__ classifier head. \n",
    "- Remember to freeze the beginning part of the model. (We will talk about this step more in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:18:24.871992Z",
     "iopub.status.busy": "2025-03-07T09:18:24.871695Z",
     "iopub.status.idle": "2025-03-07T09:18:25.402091Z",
     "shell.execute_reply": "2025-03-07T09:18:25.401121Z",
     "shell.execute_reply.started": "2025-03-07T09:18:24.871969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "\n",
    "        # Load ConvNeXt-Tiny (better than ResNet/EfficientNet)\n",
    "        self.model = models.convnext_tiny(pretrained=True)\n",
    "\n",
    "        # Unfreeze last few layers for better fine-tuning\n",
    "        for param in list(self.model.parameters())[-20:]:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Modify classifier\n",
    "        in_features = self.model.classifier[2].in_features\n",
    "        self.model.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # Global Average Pooling to remove (1,1) dims\n",
    "            nn.Flatten(),  # Flatten from [batch, features, 1, 1] → [batch, features]\n",
    "            nn.LayerNorm(in_features),  \n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(256, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 7  \n",
    "model = EmotionClassifier(num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:18:28.663701Z",
     "iopub.status.busy": "2025-03-07T09:18:28.663339Z",
     "iopub.status.idle": "2025-03-07T09:48:52.257838Z",
     "shell.execute_reply": "2025-03-07T09:48:52.256512Z",
     "shell.execute_reply.started": "2025-03-07T09:18:28.663674Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 -> Train Loss: 1.2000, Train Acc: 57.30% | Val Loss: 1.4595, Val Acc: 46.53%\n",
      "Epoch 2/20 -> Train Loss: 0.8786, Train Acc: 71.08% | Val Loss: 0.9804, Val Acc: 68.35%\n",
      "Epoch 3/20 -> Train Loss: 0.6447, Train Acc: 78.87% | Val Loss: 1.1229, Val Acc: 64.38%\n",
      "Epoch 4/20 -> Train Loss: 0.4968, Train Acc: 83.97% | Val Loss: 0.7567, Val Acc: 78.47%\n",
      "Epoch 5/20 -> Train Loss: 0.3932, Train Acc: 88.07% | Val Loss: 0.7173, Val Acc: 75.89%\n",
      "Epoch 6/20 -> Train Loss: 0.3110, Train Acc: 91.08% | Val Loss: 0.8670, Val Acc: 73.41%\n",
      "Epoch 7/20 -> Train Loss: 0.2786, Train Acc: 92.66% | Val Loss: 0.8179, Val Acc: 74.50%\n",
      "Epoch 8/20 -> Train Loss: 0.2350, Train Acc: 94.38% | Val Loss: 0.6754, Val Acc: 77.28%\n",
      "Epoch 9/20 -> Train Loss: 0.1923, Train Acc: 95.51% | Val Loss: 1.5776, Val Acc: 56.25%\n",
      "Epoch 10/20 -> Train Loss: 0.1851, Train Acc: 96.17% | Val Loss: 0.8019, Val Acc: 73.41%\n",
      "Epoch 11/20 -> Train Loss: 0.1751, Train Acc: 96.23% | Val Loss: 1.3939, Val Acc: 65.08%\n",
      "Epoch 12/20 -> Train Loss: 0.1731, Train Acc: 96.42% | Val Loss: 0.9938, Val Acc: 67.46%\n",
      "Epoch 13/20 -> Train Loss: 0.1557, Train Acc: 96.81% | Val Loss: 1.6941, Val Acc: 51.79%\n",
      "Epoch 14/20 -> Train Loss: 0.1147, Train Acc: 97.71% | Val Loss: 0.9384, Val Acc: 74.40%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-071d2dcf8cb6>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-071d2dcf8cb6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Gradient Clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy function\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(torch.argmax(y_pred, dim=1), y_true).sum().item()\n",
    "    return (correct / len(y_pred)) * 100 \n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=20, device='cuda'):\n",
    "    model.to(device)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Adjust learning rate dynamically\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += accuracy_fn(labels, outputs)  \n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc /= len(train_loader)\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_acc += accuracy_fn(labels, outputs)\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc /= len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} -> \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer with Learning Rate Scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# Train Model\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you know the rest by now! Bring in the accuracy function and the train/validate loop. \n",
    "\n",
    "Train the model and plot the loss and accuracy curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are finished training your model, simply run the following code. \n",
    "\n",
    "<span style=\"color:crimson\">Do not modify this code! </span>\n",
    "\n",
    "Except for changing the path to the __x_test.pt__ file and renaming __your_team_name.txt__ file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:48:58.112920Z",
     "iopub.status.busy": "2025-03-07T09:48:58.112622Z",
     "iopub.status.idle": "2025-03-07T09:48:58.285732Z",
     "shell.execute_reply": "2025-03-07T09:48:58.284910Z",
     "shell.execute_reply.started": "2025-03-07T09:48:58.112897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-caa0eaede9bc>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  x_test = torch.load(\"/kaggle/input/faceemotionstestdata/x_test.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([350, 3, 224, 224])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = torch.load(\"/kaggle/input/faceemotionstestdata/x_test.pt\")\n",
    "x_test.shape # torch.Size([350, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T09:49:05.680480Z",
     "iopub.status.busy": "2025-03-07T09:49:05.680155Z",
     "iopub.status.idle": "2025-03-07T09:49:07.715026Z",
     "shell.execute_reply": "2025-03-07T09:49:07.714277Z",
     "shell.execute_reply.started": "2025-03-07T09:49:05.680453Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# DO NOT CHANGE\n",
    "model.eval()\n",
    "y_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in x_test:\n",
    "        x = x.to(device).unsqueeze(0)\n",
    "        y_logits = model(x)\n",
    "        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)\n",
    "        y_preds.append(y_pred.item())\n",
    "        \n",
    "print( len(y_preds) )\n",
    "np.savetxt( \"teamavm.txt\", y_preds )\n",
    "print( \"Done!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6799199,
     "sourceId": 10934120,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6808133,
     "sourceId": 10945895,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
